# Text to Video Generation using Hugging Face

This repository contains code for generating videos from textual prompts using Hugging Face's diffusers library with the "text-to-video-ms-1.7b" model.

## Table of Contents
1. [Introduction](#introduction)
2. [Project Structure](#project-structure)
3. [Dependencies](#dependencies)
4. [Usage](#usage)
5. [Results](#results)
6. [Contributing](#contributing)
7. [License](#license)

## Introduction

The goal of this project is to explore text-to-video generation using Hugging Face's diffusers library. The "text-to-video-ms-1.7b" model is utilized to generate videos from textual prompts. This involves implementing a diffusion pipeline to integrate natural language processing and computer vision techniques for video generation.

## Project Structure

The repository has the following structure:


## Dependencies

The following dependencies are required to run the code:

- Python 3.x
- Hugging Face Transformers library
- PyTorch
- Other required libraries (specified in requirements.txt)

Install the dependencies using the following command:


## Usage

To generate videos from textual prompts, run the following command:


Replace `"Your textual prompt here"` with the desired text prompt.

## Results

The generated videos will be saved in the `results/` directory.

## Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
